#################################################################
###### FILE MODIFICATO PER LA MACCHINA DEEP
#################################################################
# pretrained weights file for generator (vuoto se NoneType)
# serve a proseguire un precedente training
netD_checkpoint : 
netG_checkpoint : 
# nome del modello. 
# questa stringa sara' parte del nome directory (si aggiunge la data e l'ora)
# che conterra' i risultati e sorgenti usati
nomeModello : "Prova_2SA"

#nomeFile per i grafici losses
nomeFileLosses : ["G_losses.csv", "D_losses.csv"]

# Root directory for dataset
dataroot : "~/Focus/Datasets/immagini/celeba"

# Number of workers for dataloader
# questo puo' essere ridotto a 1 se si hanno problemi di memoria
workers : 2

# Batch size during training 
batch_size : 64

# Spatial size of training images. All images will be resized to this
#   size using a transformer.
# Per i modelli Genb4 questo deve essere una potenza di 4
# Per gli altri  una potenza di 2
image_size : 256

# Number of channels in the training images. For color images this is 3
# da non modificare
nc : 3

# Size of z latent vector (i.e. size of generator input)
nz : 100

# Size of feature maps in generator (ngf) and discriminator (ndf)
# Il numero di kernel influisce sulla grandezza dei layer delle reti, ma non sulla
# grandezza della immagine
ngf : 128
ndf : 128
  
# Number of training epochs
num_epochs : 50

# cadenza salvataggio modelli
# Le immagini di prova sono salvate ogni epoca
cadenza_epoche : 10

# Learning rate for optimizers
lrd : 0.0002
lrg : 0.0001

# Beta1 hyperparam for Adam optimizers
# sembra che un valore accettabile per beta1 sia 0.5 
# https://arxiv.org/pdf/1511.06434.pdf

# Adam parameters
#=================
# alpha. Also referred to as the learning rate or step size. 
#        The proportion that weights are updated (e.g. 0.001). 
#        Larger values (e.g. 0.3) results in faster initial learning 
#        before the rate is updated. Smaller values (e.g. 1.0E-5) 
#        slow learning right down during training

# beta1. The exponential decay rate for the first moment estimates (e.g. 0.9).

# beta2. The exponential decay rate for the second-moment estimates (e.g. 0.999). 
#        This value should be set close to 1.0 on problems with a sparse gradient 
#        (e.g. NLP and computer vision problems).

# epsilon. Is a very small number to prevent any division by 
#          zero in the implementation (e.g. 10E-8).
#-------------------------------------------------
beta1 : 0.7
beta2 : 0.999

# Number of GPUs available. Se si usa 0 si forza il CPU mode.
# altrimenti mettere il numero di GPU da usare
ngpu : 0

# Numero di campioni da usare nel debug, se -1 allora si usano tutti i sample nel dataset
# Sare a fare prove un un subset dela dataset per training veloci
n_samples : 10000

# Number of layers log_2 (image_size)-3
# k : 2
# calcolato all'interno del programma considerando
# k = int(math.log(image_size, 2)) - 3
